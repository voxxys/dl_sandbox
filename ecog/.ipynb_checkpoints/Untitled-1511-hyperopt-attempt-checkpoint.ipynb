{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy import io\n",
    "from scipy.signal import butter, lfilter\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "from time import gmtime, strftime, time\n",
    "\n",
    "from nd_legacy import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    \n",
    "    ecog_matfile = io.loadmat('test_ecog_data.mat')\n",
    "    raw_data = ecog_matfile['raw_data']\n",
    "    raw_acc = ecog_matfile['raw_acc']\n",
    "    mv_acc = ecog_matfile['mv_acc']\n",
    "\n",
    "    sampling_rate = 2000\n",
    "\n",
    "    raw_data = raw_data[:,0:-10000]\n",
    "    raw_acc = raw_acc[:,0:-10000]\n",
    "    mv_acc = mv_acc[:,0:-10000]\n",
    "\n",
    "    len_raw_data = raw_data.shape[1]\n",
    "    \n",
    "    raw_acc_f = butter_bandpass_filter(raw_acc, 0.1, 1, sampling_rate, order=3, how_to_filt = 'separately')\n",
    "\n",
    "    idxs_train = np.arange(0,len_raw_data//3);\n",
    "    idxs_val = np.arange(len_raw_data//3,len_raw_data//2);\n",
    "    idxs_test = np.arange(len_raw_data//2,len_raw_data - 1000);\n",
    "\n",
    "    raw_data_train = raw_data[4:8,idxs_train]\n",
    "    raw_data_val = raw_data[4:8,idxs_val]\n",
    "    raw_data_test = raw_data[4:8,idxs_test]\n",
    "\n",
    "    mv_acc_train = raw_acc_f[:,idxs_train]\n",
    "    mv_acc_val = raw_acc_f[:,idxs_val]\n",
    "    mv_acc_test = raw_acc_f[:,idxs_test]\n",
    "\n",
    "    def generate_slice(slice_len, pos=2, val=False):\n",
    "        if val:\n",
    "            X = raw_data_val\n",
    "            y = mv_acc_val\n",
    "        else:\n",
    "            X = raw_data_train\n",
    "            y = mv_acc_train\n",
    "\n",
    "        len_X = X.shape[1]\n",
    "\n",
    "        while True:\n",
    "            slice_start = np.random.choice(len_X - slice_len)\n",
    "            slice_end = slice_start + slice_len\n",
    "            slice_mid = slice_start + slice_len//2\n",
    "            slice_x = X[:,slice_start:slice_end].T\n",
    "\n",
    "            if pos==0:\n",
    "                slice_y = y[0,slice_start]\n",
    "            elif pos==1:\n",
    "                slice_y = y[0,slice_mid]\n",
    "            else:            \n",
    "                slice_y = y[0,slice_end]\n",
    "\n",
    "            return slice_x, slice_y\n",
    "\n",
    "    def train_generator(batch_size, slice_len, pos=2):\n",
    "        while True:\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "\n",
    "            for i in range(0, batch_size):\n",
    "                x, y = generate_slice(slice_len, pos, val=False)\n",
    "                batch_x.append(x)\n",
    "                batch_y.append(y)\n",
    "\n",
    "            y = np.array(batch_y)\n",
    "            x = np.array([i for i in batch_x])\n",
    "            yield (x, y)\n",
    "            \n",
    "    def validation_generator(batch_size, slice_len, pos=2):\n",
    "        while True:\n",
    "            batch_x = []\n",
    "            batch_y = []\n",
    "\n",
    "            for i in range(0, batch_size):\n",
    "                x, y = generate_slice(slice_len, pos, val=True)\n",
    "                batch_x.append(x)\n",
    "                batch_y.append(y)\n",
    "\n",
    "            y = np.array(batch_y)\n",
    "            x = np.array([i for i in batch_x])\n",
    "            yield (x, y)\n",
    "\n",
    "    return train_generator, validation_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperas import optim\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas.distributions import choice, uniform, conditional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Dense, maximum, Dropout, Input, merge, GlobalMaxPooling1D, MaxPooling1D, Flatten, LSTM, BatchNormalization\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(train_generator, validation_generator):\n",
    "\n",
    "    ###\n",
    "    \n",
    "    slice_len = 2000\n",
    "\n",
    "    nb_filters = 10\n",
    "    kernel_size = 50\n",
    "\n",
    "    pos = 2\n",
    "    \n",
    "    input_seq = Input(shape=(slice_len, 4))\n",
    "\n",
    "    inputbn = BatchNormalization(axis = -1)(input_seq)\n",
    "\n",
    "    convolved = Conv1D(nb_filters, kernel_size={{choice([50, 100, 200, 300, 500, 1000])}}, padding=\"same\", activation='elu', kernel_initializer='orthogonal')(inputbn)\n",
    "    convolvedbn = BatchNormalization(axis = -1)(convolved)\n",
    "    pooled = MaxPooling1D(pool_size=4)(convolvedbn)\n",
    "\n",
    "    convolved2 = Conv1D(nb_filters*4, kernel_size={{choice([50, 100, 200, 300, 500, 1000])}}, padding=\"same\", activation='elu', kernel_initializer='orthogonal')(pooled)\n",
    "    convolved2bn = BatchNormalization(axis = -1)(convolved2)\n",
    "    pooled2 = MaxPooling1D(pool_size=4)(convolved2bn)\n",
    "\n",
    "    convolved3 = Conv1D(nb_filters*16, kernel_size={{choice([50, 100, 200, 300, 500, 1000])}}, padding=\"same\", activation='elu', kernel_initializer='orthogonal')(pooled2)\n",
    "    convolved3bn = BatchNormalization(axis = -1)(convolved3)\n",
    "    pooled3 = MaxPooling1D(pool_size=4)(convolved3bn)\n",
    "\n",
    "    convolved4 = Conv1D(nb_filters*64, kernel_size={{choice([50, 100, 200, 300, 500, 1000])}}, padding=\"same\", activation='elu', kernel_initializer='orthogonal')(pooled3)\n",
    "    convolved4bn = BatchNormalization(axis = -1)(convolved4)\n",
    "\n",
    "    flat = GlobalMaxPooling1D()(convolved4bn)\n",
    "\n",
    "    dense1 = maximum([Dense(1000, activation='linear', kernel_initializer='glorot_normal')(flat) for _ in range(3)])\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    dense1do = Dropout(0.2)(dense1)\n",
    "\n",
    "    out = Dense(1, activation='linear')(dense1do) \n",
    "    model = Model(inputs=input_seq, outputs=out)\n",
    "\n",
    "    ###\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
    "    \n",
    "    ##\n",
    "\n",
    "    #path_to_save_model = \"1511_hyper_test\"\n",
    "\n",
    "    steps_per_epoch = 600\n",
    "    validation_steps = 300\n",
    "    epochs = 200\n",
    "    patience = 20\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    #earlyStopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=2, mode='auto')\n",
    "    #checkpointer = ModelCheckpoint(path_to_save_model, monitor='val_loss', verbose=2,\n",
    "    #                               save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "    model.fit_generator(train_generator(slice_len=slice_len, batch_size=batch_size), \n",
    "                        steps_per_epoch=steps_per_epoch, \n",
    "                        epochs=epochs, \n",
    "                        #callbacks=[earlyStopping, checkpointer], \n",
    "                        verbose=1, \n",
    "                        validation_steps=validation_steps, \n",
    "                        validation_data=validation_generator(slice_len=slice_len, batch_size=batch_size, pos=pos))\n",
    "\n",
    "    score, acc = model.evaluate_generator(data_generator(slice_len=slice_len, batch_size=batch_size, \n",
    "                                                     pos=pos, val=True),500)\n",
    "\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy import io\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy.signal import butter, lfilter\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import h5py\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import random\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import datetime\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from time import gmtime, strftime, time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nd_legacy import *\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv1D, Dense, maximum, Dropout, Input, merge, GlobalMaxPooling1D, MaxPooling1D, Flatten, LSTM, BatchNormalization\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model, load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import RMSprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'kernel_size': hp.uniform('kernel_size', 0, 500),\n",
      "        'kernel_size_1': hp.uniform('kernel_size_1', 0, 500),\n",
      "        'kernel_size_2': hp.uniform('kernel_size_2', 0, 500),\n",
      "        'kernel_size_3': hp.uniform('kernel_size_3', 0, 500),\n",
      "        'activation': hp.choice('activation', ['relu', 'sigmoid']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \n",
      "   3: ecog_matfile = io.loadmat('test_ecog_data.mat')\n",
      "   4: raw_data = ecog_matfile['raw_data']\n",
      "   5: raw_acc = ecog_matfile['raw_acc']\n",
      "   6: mv_acc = ecog_matfile['mv_acc']\n",
      "   7: \n",
      "   8: sampling_rate = 2000\n",
      "   9: \n",
      "  10: raw_data = raw_data[:,0:-10000]\n",
      "  11: raw_acc = raw_acc[:,0:-10000]\n",
      "  12: mv_acc = mv_acc[:,0:-10000]\n",
      "  13: \n",
      "  14: len_raw_data = raw_data.shape[1]\n",
      "  15: \n",
      "  16: raw_acc_f = butter_bandpass_filter(raw_acc, 0.1, 1, sampling_rate, order=3, how_to_filt = 'separately')\n",
      "  17: \n",
      "  18: idxs_train = np.arange(0,len_raw_data//3);\n",
      "  19: idxs_val = np.arange(len_raw_data//3,len_raw_data//2);\n",
      "  20: idxs_test = np.arange(len_raw_data//2,len_raw_data - 1000);\n",
      "  21: \n",
      "  22: raw_data_train = raw_data[4:8,idxs_train]\n",
      "  23: raw_data_val = raw_data[4:8,idxs_val]\n",
      "  24: raw_data_test = raw_data[4:8,idxs_test]\n",
      "  25: \n",
      "  26: mv_acc_train = raw_acc_f[:,idxs_train]\n",
      "  27: mv_acc_val = raw_acc_f[:,idxs_val]\n",
      "  28: mv_acc_test = raw_acc_f[:,idxs_test]\n",
      "  29: \n",
      "  30: def generate_slice(slice_len, pos=2, val=False):\n",
      "  31:     if val:\n",
      "  32:         X = raw_data_val\n",
      "  33:         y = mv_acc_val\n",
      "  34:     else:\n",
      "  35:         X = raw_data_train\n",
      "  36:         y = mv_acc_train\n",
      "  37: \n",
      "  38:     len_X = X.shape[1]\n",
      "  39: \n",
      "  40:     while True:\n",
      "  41:         slice_start = np.random.choice(len_X - slice_len)\n",
      "  42:         slice_end = slice_start + slice_len\n",
      "  43:         slice_mid = slice_start + slice_len//2\n",
      "  44:         slice_x = X[:,slice_start:slice_end].T\n",
      "  45: \n",
      "  46:         if pos==0:\n",
      "  47:             slice_y = y[0,slice_start]\n",
      "  48:         elif pos==1:\n",
      "  49:             slice_y = y[0,slice_mid]\n",
      "  50:         else:            \n",
      "  51:             slice_y = y[0,slice_end]\n",
      "  52: \n",
      "  53:         return slice_x, slice_y\n",
      "  54: \n",
      "  55: def train_generator(batch_size, slice_len, pos=2):\n",
      "  56:     while True:\n",
      "  57:         batch_x = []\n",
      "  58:         batch_y = []\n",
      "  59: \n",
      "  60:         for i in range(0, batch_size):\n",
      "  61:             x, y = generate_slice(slice_len, pos, val=False)\n",
      "  62:             batch_x.append(x)\n",
      "  63:             batch_y.append(y)\n",
      "  64: \n",
      "  65:         y = np.array(batch_y)\n",
      "  66:         x = np.array([i for i in batch_x])\n",
      "  67:         yield (x, y)\n",
      "  68:         \n",
      "  69: def validation_generator(batch_size, slice_len, pos=2):\n",
      "  70:     while True:\n",
      "  71:         batch_x = []\n",
      "  72:         batch_y = []\n",
      "  73: \n",
      "  74:         for i in range(0, batch_size):\n",
      "  75:             x, y = generate_slice(slice_len, pos, val=True)\n",
      "  76:             batch_x.append(x)\n",
      "  77:             batch_y.append(y)\n",
      "  78: \n",
      "  79:         y = np.array(batch_y)\n",
      "  80:         x = np.array([i for i in batch_x])\n",
      "  81:         yield (x, y)\n",
      "  82: \n",
      "  83: \n",
      "  84: \n",
      "  85: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3: \n",
      "   4:     ###\n",
      "   5:     \n",
      "   6:     slice_len = 2000\n",
      "   7: \n",
      "   8:     nb_filters = 10\n",
      "   9:     kernel_size = 50\n",
      "  10: \n",
      "  11:     pos = 2\n",
      "  12:     \n",
      "  13:     input_seq = Input(shape=(slice_len, 4))\n",
      "  14: \n",
      "  15:     inputbn = BatchNormalization(axis = -1)(input_seq)\n",
      "  16: \n",
      "  17:     convolved = Conv1D(nb_filters, kernel_size=space['kernel_size'], padding=\"same\", activation='elu', kernel_initializer='orthogonal')(inputbn)\n",
      "  18:     convolvedbn = BatchNormalization(axis = -1)(convolved)\n",
      "  19:     pooled = MaxPooling1D(pool_size=4)(convolvedbn)\n",
      "  20: \n",
      "  21:     convolved2 = Conv1D(nb_filters*4, kernel_size=space['kernel_size_1'], padding=\"same\", activation='elu', kernel_initializer='orthogonal')(pooled)\n",
      "  22:     convolved2bn = BatchNormalization(axis = -1)(convolved2)\n",
      "  23:     pooled2 = MaxPooling1D(pool_size=4)(convolved2bn)\n",
      "  24: \n",
      "  25:     convolved3 = Conv1D(nb_filters*16, kernel_size=space['kernel_size_2'], padding=\"same\", activation='elu', kernel_initializer='orthogonal')(pooled2)\n",
      "  26:     convolved3bn = BatchNormalization(axis = -1)(convolved3)\n",
      "  27:     pooled3 = MaxPooling1D(pool_size=4)(convolved3bn)\n",
      "  28: \n",
      "  29:     convolved4 = Conv1D(nb_filters*64, kernel_size=space['kernel_size_3'], padding=\"same\", activation='elu', kernel_initializer='orthogonal')(pooled3)\n",
      "  30:     convolved4bn = BatchNormalization(axis = -1)(convolved4)\n",
      "  31: \n",
      "  32:     flat = GlobalMaxPooling1D()(convolved4bn)\n",
      "  33: \n",
      "  34:     dense1 = maximum([Dense(1000, activation='linear', kernel_initializer='glorot_normal')(flat) for _ in range(3)])\n",
      "  35:     dense1 = BatchNormalization()(dense1)\n",
      "  36:     dense1do = Dropout(0.2)(dense1)\n",
      "  37: \n",
      "  38:     out = Dense(1, activation=space['activation'])(dense1do) \n",
      "  39:     model = Model(inputs=input_seq, outputs=out)\n",
      "  40: \n",
      "  41:     ###\n",
      "  42: \n",
      "  43:     model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\n",
      "  44:     \n",
      "  45:     ##\n",
      "  46: \n",
      "  47:     #path_to_save_model = \"1511_hyper_test\"\n",
      "  48: \n",
      "  49:     steps_per_epoch = 600\n",
      "  50:     validation_steps = 300\n",
      "  51:     epochs = 200\n",
      "  52:     patience = 20\n",
      "  53: \n",
      "  54:     batch_size = 16\n",
      "  55: \n",
      "  56:     #earlyStopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=2, mode='auto')\n",
      "  57:     #checkpointer = ModelCheckpoint(path_to_save_model, monitor='val_loss', verbose=2,\n",
      "  58:     #                               save_best_only=True, mode='auto', period=1)\n",
      "  59: \n",
      "  60:     model.fit_generator(train_generator(slice_len=slice_len, batch_size=batch_size), \n",
      "  61:                         steps_per_epoch=steps_per_epoch, \n",
      "  62:                         epochs=epochs, \n",
      "  63:                         #callbacks=[earlyStopping, checkpointer], \n",
      "  64:                         verbose=1, \n",
      "  65:                         validation_steps=validation_steps, \n",
      "  66:                         validation_data=validation_generator(slice_len=slice_len, batch_size=batch_size, pos=pos))\n",
      "  67: \n",
      "  68:     score, acc = model.evaluate_generator(data_generator(slice_len=slice_len, batch_size=batch_size, \n",
      "  69:                                                      pos=pos, val=True),500)\n",
      "  70: \n",
      "  71:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  72: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The `kernel_size` argument must be a tuple of 1 integers. Received: 305.438154641",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-22a4c4e7cb00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                       \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                       \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                       notebook_name='Untitled-1511-hyperopt-attempt')\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluation of best performing model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/hyperas-0.4-py2.7.egg/hyperas/optim.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                      \u001b[0mfull_model_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                                      verbose=verbose)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/hyperas-0.4-py2.7.egg/hyperas/optim.pyc\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack)\u001b[0m\n\u001b[1;32m    134\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m              return_argmin=True),\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     )\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     verbose=verbose)\n\u001b[1;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/hyperopt-0.1-py2.7.egg/hyperopt/base.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtemp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[0;34m(space)\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/convolutional.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/convolutional.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kernel_size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'strides'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/utils/conv_utils.pyc\u001b[0m in \u001b[0;36mnormalize_tuple\u001b[0;34m(value, n, name)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             raise ValueError('The `' + name + '` argument must be a tuple of ' +\n\u001b[0;32m---> 30\u001b[0;31m                              str(n) + ' integers. Received: ' + str(value))\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             raise ValueError('The `' + name + '` argument must be a tuple of ' +\n",
      "\u001b[0;31mValueError\u001b[0m: The `kernel_size` argument must be a tuple of 1 integers. Received: 305.438154641"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator = data()\n",
    "\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='Untitled-1511-hyperopt-attempt')\n",
    "\n",
    "print(\"Evaluation of best performing model:\")\n",
    "\n",
    "print(best_model.evaluate(validation_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
