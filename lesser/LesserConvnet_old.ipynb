{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "from scipy.signal import butter, lfilter\n",
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafolder = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, sampling_rate, order=5):\n",
    "    nyq_freq = sampling_rate*0.5\n",
    "    low = lowcut/nyq_freq\n",
    "    high = highcut/nyq_freq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_high_low_pass(lowcut, highcut, sampling_rate, order=5):\n",
    "    nyq_freq = sampling_rate*0.5\n",
    "    lower_bound = lowcut/nyq_freq\n",
    "    higher_bound = highcut/nyq_freq\n",
    "    b_high, a_high = butter(order, lower_bound, btype='high')\n",
    "    b_low, a_low = butter(order, higher_bound, btype='low')\n",
    "    return b_high, a_high, b_low, a_low\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, sampling_rate, order=5, how_to_filt = 'separately'):\n",
    "    if how_to_filt == 'separately':\n",
    "        b_high, a_high, b_low, a_low = butter_high_low_pass(lowcut, highcut, sampling_rate, order=order)\n",
    "        y = lfilter(b_high, a_high, data)\n",
    "        y = lfilter(b_low, a_low, y)\n",
    "    elif how_to_filt == 'simultaneously':\n",
    "        b, a = butter_bandpass(lowcut, highcut, sampling_rate, order=order)\n",
    "        y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def open_eeg_mat(filename, centered=True):\n",
    "    all_data = io.loadmat(filename)\n",
    "    eeg_data = all_data['data_cur']\n",
    "    if centered:\n",
    "        eeg_data = eeg_data - np.mean(eeg_data,1)[np.newaxis].T\n",
    "        print('Data were centered: channels are zero-mean')\n",
    "    states_labels = all_data['states_cur']\n",
    "    states_codes = list(np.unique(states_labels)[:])\n",
    "    sampling_rate = all_data['srate']\n",
    "    chan_names = all_data['chan_names']\n",
    "    return eeg_data, states_labels, sampling_rate, chan_names, eeg_data.shape[0], eeg_data.shape[1], states_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 6 6 6]\n",
      "[1 1 1 ..., 6 6 6]\n",
      "[1 1 1 ..., 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "train_datas = {}\n",
    "test_datas = {}\n",
    "truetest_datas = {}\n",
    "\n",
    "def to_onehot(label):\n",
    "    labels_encoding = {1: np.array([1,0,0]), 2: np.array([0,1,0]), 6: np.array([0,0,1])}\n",
    "    return labels_encoding[label]\n",
    "\n",
    "for fname in os.listdir(datafolder):\n",
    "    filename = datafolder + fname\n",
    "    [eeg_data, states_labels, sampling_rate, chan_names, chan_numb, samp_numb, states_codes] = open_eeg_mat(filename, centered=False)\n",
    "    sampling_rate = sampling_rate[0,0]\n",
    "    eeg_data = butter_bandpass_filter(eeg_data, 0.5, 45, sampling_rate, order=5, how_to_filt = 'simultaneously')\n",
    "    \n",
    "    states_labels = states_labels[0]\n",
    "    print(states_labels)\n",
    "    states_labels = states_labels[2000:-2000]\n",
    "    eeg_data = eeg_data[:,2000:-2000]\n",
    "    \n",
    "    experiment_name = \"_\".join(fname.split(\"_\")[:-1])\n",
    "    if fname.endswith(\"_2.mat\"):\n",
    "        test_datas[experiment_name] = {\"eeg_data\": eeg_data.T, \"labels\": states_labels}\n",
    "    elif fname.endswith(\"_1.mat\"):\n",
    "        train_datas[experiment_name] = {\"eeg_data\": eeg_data.T, \"labels\": states_labels}\n",
    "    elif fname.endswith(\"_2test.mat\"):\n",
    "        truetest_datas[experiment_name] = {\"eeg_data\": eeg_data.T, \"labels\": states_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate scaling for each user, should not hurt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for key in train_datas.keys():\n",
    "    sc = StandardScaler()\n",
    "    train_datas[key][\"eeg_data\"] = sc.fit_transform(train_datas[key][\"eeg_data\"])\n",
    "    test_datas[key][\"eeg_data\"] = sc.transform(test_datas[key][\"eeg_data\"])\n",
    "    truetest_datas[key][\"eeg_data\"] = sc.transform(truetest_datas[key][\"eeg_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slice_len = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_slice(test=False):\n",
    "    if test:\n",
    "        experiment_data = random.choice(list(test_datas.values()))\n",
    "    else:\n",
    "        experiment_data = random.choice(list(train_datas.values()))\n",
    "    \n",
    "    X = experiment_data[\"eeg_data\"]\n",
    "    y = experiment_data[\"labels\"]\n",
    "    \n",
    "    while True:\n",
    "        slice_start = np.random.choice(len(X) - slice_len)\n",
    "        slice_end = slice_start + slice_len\n",
    "        slice_x = X[slice_start:slice_end]\n",
    "        #slice_x = normalize(slice_x)\n",
    "        slice_y = y[slice_start:slice_end]\n",
    "        \n",
    "        if len(set(slice_y)) == 1:\n",
    "            return slice_x, to_onehot(slice_y[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_slice()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size, test=False):\n",
    "    while True:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "            x, y = generate_slice(test=test)\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "            \n",
    "        y = np.array(batch_y)\n",
    "        x = np.array([i for i in batch_x])\n",
    "        yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D, Dense, Dropout, Input, merge, GlobalMaxPooling1D, MaxPooling1D, Flatten, LSTM\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_model(input_len, fsize):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input_seq = Input(shape=(input_len, 24))\n",
    "    nb_filters = 50\n",
    "    convolved = Convolution1D(nb_filters, 5, border_mode=\"same\", activation=\"tanh\")(input_seq)\n",
    "    pooled = GlobalMaxPooling1D()(convolved)\n",
    "    compressed = Dense(20, activation=\"linear\")(pooled)\n",
    "    compressed = Dropout(0.3)(compressed)\n",
    "    compressed = Dense(20, activation=\"relu\")(compressed)\n",
    "    compressed = Dropout(0.3)(compressed)\n",
    "    model = Model(input=input_seq, output=compressed)            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\voxxys\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(50, 5, activation=\"tanh\", padding=\"same\")`\n",
      "  \n",
      "C:\\Users\\voxxys\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"dr...)`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\voxxys\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "input1125_seq = Input(shape=(slice_len, 24))\n",
    "\n",
    "base_network1125 = get_base_model(slice_len, 10)\n",
    "\n",
    "embedding_1125 = base_network1125(input1125_seq)\n",
    "out = Dense(3, activation='softmax')(embedding_1125)\n",
    "    \n",
    "model = Model(input=input1125_seq, output=out)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\voxxys\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., 15000, 10, callbacks=[<keras.ca..., verbose=1, validation_data=<generator..., validation_steps=5000)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14996/15000 [============================>.] - ETA: 0s - loss: 0.0764 - categorical_accuracy: 0.9728Epoch 00000: val_categorical_accuracy improved from -inf to 0.72604, saving model to convlstm_alldata.h5\n",
      "15000/15000 [==============================] - 179s - loss: 0.0764 - categorical_accuracy: 0.9728 - val_loss: 1.8573 - val_categorical_accuracy: 0.7260\n",
      "Epoch 2/10\n",
      "14995/15000 [============================>.] - ETA: 0s - loss: 0.0253 - categorical_accuracy: 0.9923Epoch 00001: val_categorical_accuracy improved from 0.72604 to 0.75917, saving model to convlstm_alldata.h5\n",
      "15000/15000 [==============================] - 174s - loss: 0.0253 - categorical_accuracy: 0.9923 - val_loss: 1.5765 - val_categorical_accuracy: 0.7592\n",
      "Epoch 3/10\n",
      "14995/15000 [============================>.] - ETA: 0s - loss: 0.0211 - categorical_accuracy: 0.9938Epoch 00002: val_categorical_accuracy improved from 0.75917 to 0.80632, saving model to convlstm_alldata.h5\n",
      "15000/15000 [==============================] - 175s - loss: 0.0211 - categorical_accuracy: 0.9938 - val_loss: 0.9901 - val_categorical_accuracy: 0.8063\n",
      "Epoch 4/10\n",
      "14998/15000 [============================>.] - ETA: 0s - loss: 0.0168 - categorical_accuracy: 0.9952Epoch 00003: val_categorical_accuracy did not improve\n",
      "15000/15000 [==============================] - 171s - loss: 0.0168 - categorical_accuracy: 0.9952 - val_loss: 1.5931 - val_categorical_accuracy: 0.7709\n",
      "Epoch 5/10\n",
      "14995/15000 [============================>.] - ETA: 0s - loss: 0.0164 - categorical_accuracy: 0.9952Epoch 00004: val_categorical_accuracy did not improve\n",
      "15000/15000 [==============================] - 175s - loss: 0.0164 - categorical_accuracy: 0.9952 - val_loss: 1.2390 - val_categorical_accuracy: 0.7915\n",
      "Epoch 6/10\n",
      "14995/15000 [============================>.] - ETA: 0s - loss: 0.0129 - categorical_accuracy: 0.9962Epoch 00005: val_categorical_accuracy improved from 0.80632 to 0.88106, saving model to convlstm_alldata.h5\n",
      "15000/15000 [==============================] - 180s - loss: 0.0129 - categorical_accuracy: 0.9962 - val_loss: 0.5813 - val_categorical_accuracy: 0.8811\n",
      "Epoch 7/10\n",
      "14996/15000 [============================>.] - ETA: 0s - loss: 0.0099 - categorical_accuracy: 0.9971Epoch 00006: val_categorical_accuracy did not improve\n",
      "15000/15000 [==============================] - 176s - loss: 0.0099 - categorical_accuracy: 0.9971 - val_loss: 0.5985 - val_categorical_accuracy: 0.8263\n",
      "Epoch 8/10\n",
      "14995/15000 [============================>.] - ETA: 0s - loss: 0.0116 - categorical_accuracy: 0.9967Epoch 00007: val_categorical_accuracy did not improve\n",
      "15000/15000 [==============================] - 168s - loss: 0.0116 - categorical_accuracy: 0.9967 - val_loss: 1.3406 - val_categorical_accuracy: 0.8331\n",
      "Epoch 9/10\n",
      "14999/15000 [============================>.] - ETA: 0s - loss: 0.0079 - categorical_accuracy: 0.9978Epoch 00008: val_categorical_accuracy did not improve\n",
      "15000/15000 [==============================] - 176s - loss: 0.0079 - categorical_accuracy: 0.9978 - val_loss: 1.4844 - val_categorical_accuracy: 0.8168\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "nb_epoch = 100000\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='auto')\n",
    "checkpointer = ModelCheckpoint(\"convlstm_alldata.h5\", monitor='val_categorical_accuracy', verbose=1,\n",
    "                               save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "samples_per_epoch = 15000\n",
    "nb_epoch = 10\n",
    "\n",
    "history = model.fit_generator(data_generator(batch_size=25), samples_per_epoch, nb_epoch, \n",
    "                    callbacks=[earlyStopping, checkpointer], verbose=1, nb_val_samples=5000, \n",
    "                    validation_data=data_generator(batch_size=25, test=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_categorical_accuracy', 'loss', 'categorical_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xee15eced30>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMxb8xhgTZiz4jTEmzFjwG2NMmPn/t6poGRgYzWkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xee15110f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('convlstm_alldata.h5');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7e1bfb8048>,\n",
       " <keras.engine.training.Model at 0x7e1bf7aac8>,\n",
       " <keras.layers.core.Dense at 0x7e1c064940>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "slice index 20 of dimension 1 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [?,3], [2], [2], [2] and with computed input tensors: input[1] = <0 20>, input[2] = <0 21>, input[3] = <1 1>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    672\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: slice index 20 of dimension 1 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [?,3], [2], [2], [2] and with computed input tensors: input[1] = <0 20>, input[2] = <0 21>, input[3] = <1 1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0c2f0fb7fd1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mvis_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualize_class_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mvis_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_vis-0.3.3-py3.6.egg\\vis\\visualization.py\u001b[0m in \u001b[0;36mvisualize_class_activation\u001b[1;34m(model, layer_idx, filter_indices, seed_input, input_range, act_max_weight, lp_norm_weight, tv_weight, **optimizer_params)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mTotalVariation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtv_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     ]\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvisualize_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptimizer_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_vis-0.3.3-py3.6.egg\\vis\\visualization.py\u001b[0m in \u001b[0;36mvisualize_activation\u001b[1;34m(input_tensor, losses, seed_input, input_range, **optimizer_params)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0moptimizer_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer_params_default\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptimizer_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_vis-0.3.3-py3.6.egg\\vis\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_tensor, losses, input_range, wrt_tensor, norm_grads)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# Perf optimization. Don't build loss function with 0 weight.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[0moverall_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moverall_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moverall_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_vis-0.3.3-py3.6.egg\\vis\\losses.py\u001b[0m in \u001b[0;36mbuild_loss\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_dense\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;31m# slicer is used to deal with `channels_first` or `channels_last` image data formats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_SliceHelper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m    661\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m   3513\u001b[0m                                 \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3514\u001b[0m                                 \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3515\u001b[1;33m                                 shrink_axis_mask=shrink_axis_mask, name=name)\n\u001b[0m\u001b[0;32m   3516\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2506\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2509\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2510\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1871\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1873\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1874\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1875\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[0;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: slice index 20 of dimension 1 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [?,3], [2], [2], [2] and with computed input tensors: input[1] = <0 20>, input[2] = <0 21>, input[3] = <1 1>."
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from vis.utils import utils\n",
    "from vis.utils.vggnet import VGG16\n",
    "from vis.visualization import visualize_class_activation\n",
    "\n",
    "\n",
    "# Build the VGG16 network with ImageNet weights\n",
    "#model = VGG16(weights='imagenet', include_top=True)\n",
    "#print('Model loaded.')\n",
    "\n",
    "# The name of the layer we want to visualize\n",
    "# (see model definition in vggnet.py)\n",
    "layer_name = 'dense_6'\n",
    "layer_idx = [idx for idx, layer in enumerate(model.layers) if layer.name == layer_name][0]\n",
    "\n",
    "# Generate three different images of the same output index.\n",
    "vis_images = []\n",
    "for idx in [20, 20, 20]:\n",
    "    img = visualize_class_activation(model, layer_idx, filter_indices=idx, max_iter=500)\n",
    "    img = utils.draw_text(img, str(idx))\n",
    "    vis_images.append(img)\n",
    "\n",
    "stitched = utils.stitch_images(vis_images)    \n",
    "plt.axis('off')\n",
    "plt.imshow(stitched)\n",
    "plt.title(layer_name)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualize_class_activation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f7179b11def5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvisualize_class_activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'visualize_class_activation' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vis.losses import ActivationMaximization\n",
    "from vis.regularizers import TotalVariation, LPNorm\n",
    "from vis.modifiers import Jitter\n",
    "from vis.optimizer import Optimizer\n",
    "\n",
    "from vis.callbacks import GifGenerator\n",
    "from vis.utils.vggnet import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 500, 24)           0         \n",
      "_________________________________________________________________\n",
      "model_3 (Model)              (None, 20)                7490      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 7,553\n",
      "Trainable params: 7,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_name = 'dense_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "slice index 20 of dimension 1 out of bounds. for 'strided_slice_1' (op: 'StridedSlice') with input shapes: [?,3], [2], [2], [2] and with computed input tensors: input[1] = <0 20>, input[2] = <0 21>, input[3] = <1 1>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    672\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: slice index 20 of dimension 1 out of bounds. for 'strided_slice_1' (op: 'StridedSlice') with input shapes: [?,3], [2], [2], [2] and with computed input tensors: input[1] = <0 20>, input[2] = <0 21>, input[3] = <1 1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-6a8e44dce3c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mTotalVariation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_modifiers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mJitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mGifGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'opt_progress'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\vis\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, img_input, losses, wrt, norm_grads)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m# Perf optimization. Don't build loss function with 0 weight.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0moverall_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moverall_loss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0moverall_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\vis\\losses.py\u001b[0m in \u001b[0;36mbuild_loss\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_dense\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;31m# slicer is used to deal with theano/tensorflow without the ugly conditional statements.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_SliceHelper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m    661\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m   3513\u001b[0m                                 \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3514\u001b[0m                                 \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3515\u001b[1;33m                                 shrink_axis_mask=shrink_axis_mask, name=name)\n\u001b[0m\u001b[0;32m   3516\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2506\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2509\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2510\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1871\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1873\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1874\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1875\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[0;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[0;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: slice index 20 of dimension 1 out of bounds. for 'strided_slice_1' (op: 'StridedSlice') with input shapes: [?,3], [2], [2], [2] and with computed input tensors: input[1] = <0 20>, input[2] = <0 21>, input[3] = <1 1>."
     ]
    }
   ],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "output_class = [20]\n",
    "\n",
    "losses = [\n",
    "    (ActivationMaximization(layer_dict[layer_name], output_class), 2),\n",
    "    (LPNorm(model.input), 10),\n",
    "    (TotalVariation(model.input), 10)\n",
    "]\n",
    "opt = Optimizer(model.input, losses)\n",
    "opt.minimize(max_iter=500, verbose=True, image_modifiers=[Jitter()], callbacks=[GifGenerator('opt_progress')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ..., 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/ksenia_long_2test'\n",
    "#filename = 'data/ksenia_long_1'\n",
    "[eeg_data, states_labels, sampling_rate, chan_names, chan_numb, samp_numb, states_codes] = open_eeg_mat(filename, centered=False)\n",
    "sampling_rate = sampling_rate[0,0]\n",
    "eeg_data = butter_bandpass_filter(eeg_data, 0.5, 45, sampling_rate, order=5, how_to_filt = 'simultaneously')\n",
    "\n",
    "states_labels = states_labels[0]\n",
    "print(states_labels)\n",
    "states_labels = states_labels[2000:-2000]\n",
    "eeg_data = eeg_data[:,2000:-2000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ksenia_long': {'eeg_data': array([[  0.72886774,   0.28953301,  -0.06310475, ...,  -0.84023706,\n",
       "           -0.5951511 ,  14.67372102],\n",
       "         [  0.40641331,   0.01874414,  -0.37925777, ...,  -0.82688436,\n",
       "           -0.5592863 ,  15.00637087],\n",
       "         [  0.09576368,  -0.24521609,  -0.66651021, ...,  -0.71990772,\n",
       "           -0.42063431,  15.3341146 ],\n",
       "         ..., \n",
       "         [ -0.49877598,  -0.79176197,  -1.56604115, ...,   0.55407907,\n",
       "            0.35563988,  -0.02974017],\n",
       "         [ -0.07212109,  -0.34952697,  -1.31718183, ...,   0.39177637,\n",
       "            0.15897734,  -0.02973893],\n",
       "         [  0.38851491,   0.19016002,  -0.7966039 , ...,   0.11234432,\n",
       "           -0.13938467,  -0.02973764]]),\n",
       "  'labels': array([1, 1, 1, ..., 6, 6, 6], dtype=uint8)}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truetest_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_data = truetest_datas['ksenia_long']['eeg_data'].T\n",
    "idxs = np.arange(0,eeg_data.shape[1],500)\n",
    "num_chunks = idxs.shape[0]\n",
    "num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunks = np.zeros([num_chunks-1,24,500])\n",
    "preds = np.zeros([num_chunks-1,3])\n",
    "\n",
    "for i in range(num_chunks-1):\n",
    "    #print(idxs[i])\n",
    "    chunk = eeg_data[:,idxs[i]:(idxs[i]+500)].T\n",
    "    chunks[i,:,:] = chunk.T\n",
    "    preds[i] = model.predict(chunk[None,:,:,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xe3040dc4e0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe3044495c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.argmax(preds,axis=1)\n",
    "\n",
    "\n",
    "a_bin = np.zeros([3,a.shape[0]])\n",
    "\n",
    "a_bin[0,:] = (a == 0)\n",
    "a_bin[1,:] = (a == 1)\n",
    "a_bin[2,:] = (a == 2)\n",
    "              \n",
    "plt.imshow(a_bin, extent=[0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels = eeg_data = truetest_datas['ksenia_long']['labels']\n",
    "true_labels = true_labels[::500]\n",
    "true_labels = true_labels[0:-1]\n",
    "true_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 126)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_bin = np.zeros([3,true_labels.shape[0]])\n",
    "true_labels_bin.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xe3041566a0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDIOkhmGQ1DAMkhr/C4NVVzM5fIM9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe3040ed4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "true_labels_bin[0,:] = (true_labels == 1)\n",
    "true_labels_bin[1,:] = (true_labels == 2)\n",
    "true_labels_bin[2,:] = (true_labels == 6)\n",
    "\n",
    "plt.imshow(true_labels_bin, extent=[0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score1 = roc_auc_score(true_labels_bin,a_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80952380952380953"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predsT = preds.T;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score2 = roc_auc_score(true_labels_bin,predsT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86111111111111116"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_bin[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_bin[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 126)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(preds[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 2, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 2, 2, 2, 1, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xe303e796d8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe303bbc588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preds, extent=[0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
